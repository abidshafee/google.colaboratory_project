{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing (NLP)- TEXT Manipulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9FcChqLoIXleVOmXVOmFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abidshafee/google.colaboratory_projects/blob/master/Natural_Language_Processing_(NLP)_TEXT_Manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClrjGWcqlTRV",
        "colab_type": "text"
      },
      "source": [
        "## NLP In Python\n",
        "**Python Libraries for Data NLP**: *pandas, sklearn, renltk, TextBlob, gensim* **Statistical Analysis** and  **Math** operation such as: cleaning data, **EDA or Exploratory Data Analysis** *for word count*, and Finally **NLP** *for sentiment analysis, topic modeling, and text generation. * **Next Communication** - **Design** *inclludes scope, visualization, extract insight*, **Domain** *Expertise*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Ruf819n4pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Science Approaches\n",
        "# 1 - raise a question \n",
        "# 2 - data gathering (web scraping) and cleaning\n",
        "#       python library involves in webscraping are: requests (make http request get data from the web)\n",
        "#       beautiful soup (parse html documents extract parts of a website)\n",
        "#       pickle (serialize objects and save the data)\n",
        "#       For cleaning pandas dataframe\n",
        "# 3 - eda or Exploratory Data Analysis\n",
        "# 4 - techniques\n",
        "# 5 - insights"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcymGKp1OWx1",
        "colab_type": "text"
      },
      "source": [
        "## Web Scraping - Data Gathering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KkGT6lWstiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from urllib.request import urlopen as uReq\n",
        "import requests as uReq\n",
        "from bs4 import BeautifulSoup as soup"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqP_7PXSPNnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "776e14ad-a3c3-4483-8304-e821c861881d"
      },
      "source": [
        "live_price_url = 'https://finance.yahoo.com/quote/EURUSD=X?p=EURUSD=X&.tsrc=fin-srch'\n",
        "headers = {\n",
        "    \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
        "}\n",
        "news_url = 'https://finance.yahoo.com/news/economic-data-puts-loonie-focus-022354695.html'\n",
        "\n",
        "# opening connection and grabing webdata\n",
        "#webClient = uReq(test_url)\n",
        "\n",
        "webClient = uReq.get(live_price_url, headers=headers)\n",
        "# parsing the html\n",
        "webPage = soup(webClient.text, 'lxml')\n",
        "\n",
        "# find specific content in webpage\n",
        "\n",
        "\"\"\"\n",
        "the find() method returns only the first tag that matches the argument\n",
        "\"\"\"\n",
        "\n",
        "# find_all returns a list of all the tags that matches the argument\n",
        "m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\n",
        "# main_div = content.div.article\n",
        "# article = main_div.article\n",
        "\n",
        "# para = content.find('div', class_='canvas-body Wow(bw) Cl(start) Mb(20px) Lh(30px) Fz(18px) C(#000) D(i)')\n",
        "\n",
        "# find a specific class in a webpage\n",
        "# classContent = content.find('p', class_='canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm')\n",
        "# this code will return div with currency class\n",
        "# this way we can find other attributes such as ids as well\n",
        "\n",
        "# print(content.prettify())\n",
        "# print(classContent.text)\n",
        "# print(main_div)\n",
        "print('eurusd-now: ', m_content_live_price)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eurusd-now:  1.1637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfhX1mn_Bc0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e8aa5d3b-1910-4bf3-c5c9-85764c50dcca"
      },
      "source": [
        "# put this live price tracker in a function\n",
        "'''\n",
        "def price_tracker():\n",
        "  m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\n",
        "  print('eurusd-now: ', m_content_live_price)\n",
        "while(True):\n",
        "  price_tracker()\n",
        "'''"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef price_tracker():\\n  m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\\n  print('eurusd-now: ', m_content_live_price)\\nwhile(True):\\n  price_tracker()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjRMIpaVahpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "webClient2 = uReq.get(news_url)\n",
        "newsPage = soup(webClient2.text)\n",
        "\n",
        "news_article = newsPage.find_all('article', {'itemprop' :'articleBody'})[0].prettify()\n",
        "# print(news_article)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCSp_hdbS5Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accessing div that contain all paragraph in article tag\n",
        "article = newsPage.find_all('div', {'class': 'canvas-body Wow(bw) Cl(start) Mb(20px) Fz(16px) Lh(1.6) Ff(yahooSansFinanceFont) D(i)'})[0]\n",
        "  # headline = news_article.find('div').text\n",
        "  # print(headline.h2)\n",
        "  # para = article.p\n",
        "# print(article.prettify())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5n76d5xB8Ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "8ed817ce-0a73-4c61-d7df-2ad24491ffd4"
      },
      "source": [
        "print(article.h2.text)\n",
        "sentences = []\n",
        "# now accessing all paragraph from the div we accessed before\n",
        "for para in article.find_all('p'):\n",
        "  sentences.append(para.text)\n",
        "  # print(para.text)\n",
        "  # print()\n",
        "print(sentences)\n",
        "print('Total number of Sentences: ' + str(len(sentences)))\n",
        "print('The last Sentence is >> ' + sentences[-1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Earlier in the Day:\n",
            "['It was a relatively busy start to the day on the economic calendar. The Japanese Yen was back in action along with the Aussie Dollar.', 'Away from the economic calendar, riskier assets found early support on news of further progress towards a COVID-19 vaccine.', 'On Monday, the number of new coronavirus cases rose by 182,589 to 14,850,887. On Sunday, the number of new cases had risen by 246,207. The daily increase was lower than Saturday’s rise and 199,164 new cases from the previous Monday.', 'Germany, Italy, and Spain reported 5,413 new cases on Monday, which was up from 491 new cases on Sunday. On the previous Monday, just 2,700 new cases had been reported.', 'From the U.S, the total number of cases rose by 62,790 to 3,961,429 on Monday. On Sunday, the total number of cases had increased by 65,368. On Monday, 20th July, a total of 65,488 new cases had been reported.', 'In June, core consumer prices remained unchanged compared with June 2019. Economists had forecast a 0.1% decline.', 'According to figures released by the Ministry of Internal Affairs and Communication., the annual rate of inflation held steady at 0.1%.', 'The Japanese Yen moved from ¥107.243 to ¥107.231 upon release of the minutes and stats. At the time of writing, the Japanese Yen was up by 0.07% to ¥107.19 against the U.S Dollar.', 'The RBA Meeting Minutes were in focus in the late part of the Asian session.', 'Salient points from the minutes included:', 'The Aussie Dollar moved from $0.70267 to $0.70217 upon the release of the minutes. At the time of writing, the Aussie Dollar was up by 0.11% to $0.7024.', 'At the time of writing, the Kiwi Dollar was down by 0.02% to $0.6576.', 'It’s a particularly quiet day ahead on the economic calendar. There are no material stats due out of the Eurozone to provide the EUR with direction.', 'The lack of stats will leave the EUR in the hands of updates from the EU Recovery Fund talks and market risk sentiment.', 'While the EU Recovery Fund would be a boost for the struggling economies of Italy and Spain, the coronavirus remains a threat. Progress towards a COVID-19 vaccine, however, has continued to ease market jitters over the virus.', 'At the time of writing, the EUR was up by 0.10% to $1.1459.', 'It’s another particularly quiet day ahead on the economic calendar. There are no material stats due out of the UK to provide the Pound with direction.', 'Brexit remains in focus and will continue to limit any upside in the Pound, barring the announcement of an agreement on trade.', 'At the time of writing, the Pound was up by 0.11% to $1.2675.', 'It’s also a particularly quiet day ahead for the U.S Dollar. There are no material stats due out to provide the Greenback with direction.', 'A lack of stats will leave the Dollar in the hands of updates on COVID-19 and chatter from Washington.', 'At the time of writing, the Dollar Spot Index was down by 0.13% to 95.705.', 'It’s a busier day ahead on the economic calendar. May retail sales and June new house price figures are due out of Canada later today.', 'Barring a “risk-off” event, we would expect the retail sales figures to have the greatest impact.', 'The markets will be looking for a rebound from April’s tumble.', 'Away from the stats, any rise in tension between the U.S and China will need consideration along with COVID-19 updates.', 'At the time of writing, the Loonie was up by 0.13% to C$1.3518 against the U.S Dollar.', 'For a look at all of today’s economic events, check out our\\xa0economic calendar.', 'This article was originally posted on FX Empire']\n",
            "Total number of Sentences: 29\n",
            "The last Sentence is >> This article was originally posted on FX Empire\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TTaUA22fpqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3bedff13-0332-4839-fd2b-36e7830649f0"
      },
      "source": [
        "# creating a jiant string of sentences\n",
        "for sentence in sentences:\n",
        "  combined_sentences = ''.join(sentences)\n",
        "\n",
        "print(combined_sentences)\n",
        "combined_sentences[:10]\n",
        "print(len(combined_sentences))\n",
        "\n",
        "# accessing index\n",
        "print(combined_sentences.index('FX Empire'))\n",
        "combined_sentences[3562:-2]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It was a relatively busy start to the day on the economic calendar. The Japanese Yen was back in action along with the Aussie Dollar.Away from the economic calendar, riskier assets found early support on news of further progress towards a COVID-19 vaccine.On Monday, the number of new coronavirus cases rose by 182,589 to 14,850,887. On Sunday, the number of new cases had risen by 246,207. The daily increase was lower than Saturday’s rise and 199,164 new cases from the previous Monday.Germany, Italy, and Spain reported 5,413 new cases on Monday, which was up from 491 new cases on Sunday. On the previous Monday, just 2,700 new cases had been reported.From the U.S, the total number of cases rose by 62,790 to 3,961,429 on Monday. On Sunday, the total number of cases had increased by 65,368. On Monday, 20th July, a total of 65,488 new cases had been reported.In June, core consumer prices remained unchanged compared with June 2019. Economists had forecast a 0.1% decline.According to figures released by the Ministry of Internal Affairs and Communication., the annual rate of inflation held steady at 0.1%.The Japanese Yen moved from ¥107.243 to ¥107.231 upon release of the minutes and stats. At the time of writing, the Japanese Yen was up by 0.07% to ¥107.19 against the U.S Dollar.The RBA Meeting Minutes were in focus in the late part of the Asian session.Salient points from the minutes included:The Aussie Dollar moved from $0.70267 to $0.70217 upon the release of the minutes. At the time of writing, the Aussie Dollar was up by 0.11% to $0.7024.At the time of writing, the Kiwi Dollar was down by 0.02% to $0.6576.It’s a particularly quiet day ahead on the economic calendar. There are no material stats due out of the Eurozone to provide the EUR with direction.The lack of stats will leave the EUR in the hands of updates from the EU Recovery Fund talks and market risk sentiment.While the EU Recovery Fund would be a boost for the struggling economies of Italy and Spain, the coronavirus remains a threat. Progress towards a COVID-19 vaccine, however, has continued to ease market jitters over the virus.At the time of writing, the EUR was up by 0.10% to $1.1459.It’s another particularly quiet day ahead on the economic calendar. There are no material stats due out of the UK to provide the Pound with direction.Brexit remains in focus and will continue to limit any upside in the Pound, barring the announcement of an agreement on trade.At the time of writing, the Pound was up by 0.11% to $1.2675.It’s also a particularly quiet day ahead for the U.S Dollar. There are no material stats due out to provide the Greenback with direction.A lack of stats will leave the Dollar in the hands of updates on COVID-19 and chatter from Washington.At the time of writing, the Dollar Spot Index was down by 0.13% to 95.705.It’s a busier day ahead on the economic calendar. May retail sales and June new house price figures are due out of Canada later today.Barring a “risk-off” event, we would expect the retail sales figures to have the greatest impact.The markets will be looking for a rebound from April’s tumble.Away from the stats, any rise in tension between the U.S and China will need consideration along with COVID-19 updates.At the time of writing, the Loonie was up by 0.13% to C$1.3518 against the U.S Dollar.For a look at all of today’s economic events, check out our economic calendar.This article was originally posted on FX Empire\n",
            "3454\n",
            "3445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHXmqwwvpbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "currencies = ['Yen', 'Aussie Dollar', 'Dollar']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BskgYdK26ii5",
        "colab_type": "text"
      },
      "source": [
        "### word tokenize and removing stop words - data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqwvPvx-B-Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk as nt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize as tk"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svbP5AcUtC6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "bd23e766-ca08-484e-f800-135bbbc01435"
      },
      "source": [
        "nt.download('punkt')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGK5RvyqcwJu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "909716d6-2c82-477d-a313-31df023173cb"
      },
      "source": [
        "# converting senetnce[] object to string\n",
        "sentences = str(sentences)\n",
        "# -----------------------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokenized_words = tk(sentences)\n",
        "print(tokenized_words)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b3821f9d70a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtokenized_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-yDETTpqJv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter stopwords in the sentence\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in tokenized_words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyKrm-cc5WUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another way of filtering that will work the same\n",
        "filtered_sentence = [w for w in tokenized_words if not w in stop_words]\n",
        "print(filtered_sentence)\n",
        "print(len(filtered_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXWBhD46pP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_sentence[1:480]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKp0zwdIIjW1",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "perpose of stemming is really depends, we really don't always need to -\n",
        "stemm the tokenized words. it helps finding similar words with different forms "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWWMzjyh73Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II3leXR5Jmls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = PorterStemmer()\n",
        "stemmed_word = []\n",
        "for w in filtered_sentence:\n",
        "  stemmed_word.append(ps.stem(w))\n",
        "\n",
        "print(stemmed_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fb0x-zbTCKq",
        "colab_type": "text"
      },
      "source": [
        "### Part of Speech Taging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCvVFC4GLGgI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}