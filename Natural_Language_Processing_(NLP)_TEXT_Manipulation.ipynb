{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Language Processing (NLP)- TEXT Manipulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsOC6OoQd8InQLfqBIreMZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abidshafee/google.colaboratory_projects/blob/master/Natural_Language_Processing_(NLP)_TEXT_Manipulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClrjGWcqlTRV",
        "colab_type": "text"
      },
      "source": [
        "## NLP In Python\n",
        "**Python Libraries for Data NLP**: *pandas, sklearn, renltk, TextBlob, gensim* **Statistical Analysis** and  **Math** operation such as: cleaning data, **EDA or Exploratory Data Analysis** *for word count*, and Finally **NLP** *for sentiment analysis, topic modeling, and text generation. * **Next Communication** - **Design** *inclludes scope, visualization, extract insight*, **Domain** *Expertise*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Ruf819n4pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Science Approaches\n",
        "# 1 - raise a question \n",
        "# 2 - data gathering (web scraping) and cleaning\n",
        "#       python library involves in webscraping are: requests (make http request get data from the web)\n",
        "#       beautiful soup (parse html documents extract parts of a website)\n",
        "#       pickle (serialize objects and save the data)\n",
        "#       For cleaning pandas dataframe\n",
        "# 3 - eda or Exploratory Data Analysis\n",
        "# 4 - techniques\n",
        "# 5 - insights"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcymGKp1OWx1",
        "colab_type": "text"
      },
      "source": [
        "## Web Scraping - Data Gathering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KkGT6lWstiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from urllib.request import urlopen as uReq\n",
        "import requests as uReq\n",
        "from bs4 import BeautifulSoup as soup"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqP_7PXSPNnZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b1c5e7aa-afe1-4ba8-e5d4-3a50f0a75436"
      },
      "source": [
        "live_price_url = 'https://finance.yahoo.com/quote/EURUSD=X?p=EURUSD=X&.tsrc=fin-srch'\n",
        "headers = {\n",
        "    \"User-Agent\": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36'\n",
        "}\n",
        "news_url = 'https://finance.yahoo.com/news/economic-data-puts-loonie-focus-022354695.html'\n",
        "\n",
        "# opening connection and grabing webdata\n",
        "#webClient = uReq(test_url)\n",
        "\n",
        "webClient = uReq.get(live_price_url, headers=headers)\n",
        "# parsing the html\n",
        "webPage = soup(webClient.text, 'lxml')\n",
        "\n",
        "# find specific content in webpage\n",
        "\n",
        "\"\"\"\n",
        "the find() method returns only the first tag that matches the argument\n",
        "\"\"\"\n",
        "\n",
        "# find_all returns a list of all the tags that matches the argument\n",
        "m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\n",
        "# main_div = content.div.article\n",
        "# article = main_div.article\n",
        "\n",
        "# para = content.find('div', class_='canvas-body Wow(bw) Cl(start) Mb(20px) Lh(30px) Fz(18px) C(#000) D(i)')\n",
        "\n",
        "# find a specific class in a webpage\n",
        "# classContent = content.find('p', class_='canvas-atom canvas-text Mb(1.0em) Mb(0)--sm Mt(0.8em)--sm')\n",
        "# this code will return div with currency class\n",
        "# this way we can find other attributes such as ids as well\n",
        "\n",
        "# print(content.prettify())\n",
        "# print(classContent.text)\n",
        "# print(main_div)\n",
        "print('eurusd-now: ', m_content_live_price)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eurusd-now:  1.1783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfhX1mn_Bc0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "06636cde-b9df-4e87-f1b0-c810789cca3e"
      },
      "source": [
        "# put this live price tracker in a function\n",
        "'''\n",
        "def price_tracker():\n",
        "  m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\n",
        "  print('eurusd-now: ', m_content_live_price)\n",
        "while(True):\n",
        "  price_tracker()\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef price_tracker():\\n  m_content_live_price = webPage.find_all('div', {'class':'My(6px) Pos(r) smartphone_Mt(6px)'})[0].find('span').text\\n  print('eurusd-now: ', m_content_live_price)\\nwhile(True):\\n  price_tracker()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjRMIpaVahpe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "webClient2 = uReq.get(news_url)\n",
        "newsPage = soup(webClient2.text)\n",
        "\n",
        "news_article = newsPage.find_all('article', {'itemprop' :'articleBody'})[0].prettify()\n",
        "# print(news_article)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCSp_hdbS5Nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accessing div that contain all paragraph in article tag\n",
        "article = newsPage.find_all('div', {'class': 'canvas-body Wow(bw) Cl(start) Mb(20px) Fz(16px) Lh(1.6) Ff(yahooSansFinanceFont) D(i)'})[0]\n",
        "  # headline = news_article.find('div').text\n",
        "  # print(headline.h2)\n",
        "  # para = article.p\n",
        "# print(article.prettify())"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5n76d5xB8Ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "fb9ef779-29c5-4d73-ccf6-67df28d07704"
      },
      "source": [
        "print(article.h2.text)\n",
        "sentences = []\n",
        "# now accessing all paragraph from the div we accessed before\n",
        "for para in article.find_all('p'):\n",
        "  sentences.append(para.text)\n",
        "  # print(para.text)\n",
        "  # print()\n",
        "print(sentences)\n",
        "print('Total number of Sentences: ' + str(len(sentences)))\n",
        "print('The last Sentence is >> ' + sentences[-1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Earlier in the Day:\n",
            "['It was a relatively busy start to the day on the economic calendar. The Japanese Yen was back in action along with the Aussie Dollar.', 'Away from the economic calendar, riskier assets found early support on news of further progress towards a COVID-19 vaccine.', 'On Monday, the number of new coronavirus cases rose by 182,589 to 14,850,887. On Sunday, the number of new cases had risen by 246,207. The daily increase was lower than Saturday’s rise and 199,164 new cases from the previous Monday.', 'Germany, Italy, and Spain reported 5,413 new cases on Monday, which was up from 491 new cases on Sunday. On the previous Monday, just 2,700 new cases had been reported.', 'From the U.S, the total number of cases rose by 62,790 to 3,961,429 on Monday. On Sunday, the total number of cases had increased by 65,368. On Monday, 20th July, a total of 65,488 new cases had been reported.', 'In June, core consumer prices remained unchanged compared with June 2019. Economists had forecast a 0.1% decline.', 'According to figures released by the Ministry of Internal Affairs and Communication., the annual rate of inflation held steady at 0.1%.', 'The Japanese Yen moved from ¥107.243 to ¥107.231 upon release of the minutes and stats. At the time of writing, the Japanese Yen was up by 0.07% to ¥107.19 against the U.S Dollar.', 'The RBA Meeting Minutes were in focus in the late part of the Asian session.', 'Salient points from the minutes included:', 'The Aussie Dollar moved from $0.70267 to $0.70217 upon the release of the minutes. At the time of writing, the Aussie Dollar was up by 0.11% to $0.7024.', 'At the time of writing, the Kiwi Dollar was down by 0.02% to $0.6576.', 'It’s a particularly quiet day ahead on the economic calendar. There are no material stats due out of the Eurozone to provide the EUR with direction.', 'The lack of stats will leave the EUR in the hands of updates from the EU Recovery Fund talks and market risk sentiment.', 'While the EU Recovery Fund would be a boost for the struggling economies of Italy and Spain, the coronavirus remains a threat. Progress towards a COVID-19 vaccine, however, has continued to ease market jitters over the virus.', 'At the time of writing, the EUR was up by 0.10% to $1.1459.', 'It’s another particularly quiet day ahead on the economic calendar. There are no material stats due out of the UK to provide the Pound with direction.', 'Brexit remains in focus and will continue to limit any upside in the Pound, barring the announcement of an agreement on trade.', 'At the time of writing, the Pound was up by 0.11% to $1.2675.', 'It’s also a particularly quiet day ahead for the U.S Dollar. There are no material stats due out to provide the Greenback with direction.', 'A lack of stats will leave the Dollar in the hands of updates on COVID-19 and chatter from Washington.', 'At the time of writing, the Dollar Spot Index was down by 0.13% to 95.705.', 'It’s a busier day ahead on the economic calendar. May retail sales and June new house price figures are due out of Canada later today.', 'Barring a “risk-off” event, we would expect the retail sales figures to have the greatest impact.', 'The markets will be looking for a rebound from April’s tumble.', 'Away from the stats, any rise in tension between the U.S and China will need consideration along with COVID-19 updates.', 'At the time of writing, the Loonie was up by 0.13% to C$1.3518 against the U.S Dollar.', 'For a look at all of today’s economic events, check out our\\xa0economic calendar.', 'This article was originally posted on FX Empire']\n",
            "Total number of Sentences: 29\n",
            "The last Sentence is >> This article was originally posted on FX Empire\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TTaUA22fpqq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "814ab255-8d62-498a-c24d-4d0a42612af9"
      },
      "source": [
        "# creating a jiant string of sentences\n",
        "for sentence in sentences:\n",
        "  combined_sentences = ''.join(sentences)\n",
        "\n",
        "print(combined_sentences)\n",
        "# combined_sentences[:10]\n",
        "print(len(combined_sentences))\n",
        "\n",
        "# accessing index\n",
        "print(combined_sentences.index('FX Empire'))\n",
        "\n",
        "combined_sentences[3407:]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It was a relatively busy start to the day on the economic calendar. The Japanese Yen was back in action along with the Aussie Dollar.Away from the economic calendar, riskier assets found early support on news of further progress towards a COVID-19 vaccine.On Monday, the number of new coronavirus cases rose by 182,589 to 14,850,887. On Sunday, the number of new cases had risen by 246,207. The daily increase was lower than Saturday’s rise and 199,164 new cases from the previous Monday.Germany, Italy, and Spain reported 5,413 new cases on Monday, which was up from 491 new cases on Sunday. On the previous Monday, just 2,700 new cases had been reported.From the U.S, the total number of cases rose by 62,790 to 3,961,429 on Monday. On Sunday, the total number of cases had increased by 65,368. On Monday, 20th July, a total of 65,488 new cases had been reported.In June, core consumer prices remained unchanged compared with June 2019. Economists had forecast a 0.1% decline.According to figures released by the Ministry of Internal Affairs and Communication., the annual rate of inflation held steady at 0.1%.The Japanese Yen moved from ¥107.243 to ¥107.231 upon release of the minutes and stats. At the time of writing, the Japanese Yen was up by 0.07% to ¥107.19 against the U.S Dollar.The RBA Meeting Minutes were in focus in the late part of the Asian session.Salient points from the minutes included:The Aussie Dollar moved from $0.70267 to $0.70217 upon the release of the minutes. At the time of writing, the Aussie Dollar was up by 0.11% to $0.7024.At the time of writing, the Kiwi Dollar was down by 0.02% to $0.6576.It’s a particularly quiet day ahead on the economic calendar. There are no material stats due out of the Eurozone to provide the EUR with direction.The lack of stats will leave the EUR in the hands of updates from the EU Recovery Fund talks and market risk sentiment.While the EU Recovery Fund would be a boost for the struggling economies of Italy and Spain, the coronavirus remains a threat. Progress towards a COVID-19 vaccine, however, has continued to ease market jitters over the virus.At the time of writing, the EUR was up by 0.10% to $1.1459.It’s another particularly quiet day ahead on the economic calendar. There are no material stats due out of the UK to provide the Pound with direction.Brexit remains in focus and will continue to limit any upside in the Pound, barring the announcement of an agreement on trade.At the time of writing, the Pound was up by 0.11% to $1.2675.It’s also a particularly quiet day ahead for the U.S Dollar. There are no material stats due out to provide the Greenback with direction.A lack of stats will leave the Dollar in the hands of updates on COVID-19 and chatter from Washington.At the time of writing, the Dollar Spot Index was down by 0.13% to 95.705.It’s a busier day ahead on the economic calendar. May retail sales and June new house price figures are due out of Canada later today.Barring a “risk-off” event, we would expect the retail sales figures to have the greatest impact.The markets will be looking for a rebound from April’s tumble.Away from the stats, any rise in tension between the U.S and China will need consideration along with COVID-19 updates.At the time of writing, the Loonie was up by 0.13% to C$1.3518 against the U.S Dollar.For a look at all of today’s economic events, check out our economic calendar.This article was originally posted on FX Empire\n",
            "3454\n",
            "3445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This article was originally posted on FX Empire'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEjN6O7iU1Uw",
        "colab_type": "text"
      },
      "source": [
        "## Pickling Object\n",
        "we pickle object generally larger object for faster access and manipulate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObxoEZDJU0gs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "92d9c7f6-6fd6-4aff-e573-062c96d82ee3"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "# simple dictionary\n",
        "sm_dict = {i : np.random.randn() for i in range(7) }\n",
        "\n",
        "#-------------- Pickling ------------------------------------\n",
        "'''\n",
        "# opening pickle out as write as byte\n",
        "pickle_out = open('dict.pickle', 'wb')\n",
        "# now dumping the simple dictionary into pickle_out\n",
        "pickle.dump(sm_dict, pickle_out)\n",
        "# always close the pickle file\n",
        "pickle_out.close()\n",
        "'''\n",
        "# --------------- depickling: loading the pickle back in -------------------------\n",
        "'''\n",
        "# load dict.pickle object in pickle_in\n",
        "pickle_in = open('dict.pickle', 'rb')\n",
        "# now load back the dict.pickle into sm_dict\n",
        "sm_dict = pickle.load(pickle_in)\n",
        "print(sm_dict)\n",
        "'''\n",
        "\n",
        "# -------- first comment out the depickling part and pickle any object -----------\n",
        "# --then comment out the pickling part and depickle the dump object into original object---"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# load dict.pickle object in pickle_in\\npickle_in = open('dict.pickle', 'rb')\\n# now load back the dict.pickle into sm_dict\\nsm_dict = pickle.load(pickle_in)\\nprint(sm_dict)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbaJxbNHZFI_",
        "colab_type": "text"
      },
      "source": [
        "## Python Regular Expression py RegEx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNHXmqwwvpbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "currencies = ['Yen', 'Aussie Dollar', 'Dollar']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BskgYdK26ii5",
        "colab_type": "text"
      },
      "source": [
        "### word tokenize and removing stop words - data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqwvPvx-B-Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk as nt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize as tk"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svbP5AcUtC6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b6959915-0f21-4109-b411-20396f2e0aa5"
      },
      "source": [
        "nt.download('punkt')\n",
        "nt.download('stopwords')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGK5RvyqcwJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# converting senetnce[] object to string\n",
        "sentences = str(sentences)\n",
        "# -----------------------------------------------\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokenized_words = tk(sentences)\n",
        "print(tokenized_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-yDETTpqJv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filter stopwords in the sentence\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in tokenized_words:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "print(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyKrm-cc5WUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another way of filtering that will work the same\n",
        "filtered_sentence = [w for w in tokenized_words if not w in stop_words]\n",
        "print(filtered_sentence)\n",
        "print(len(filtered_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXWBhD46pP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filtered_sentence[1:480]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKp0zwdIIjW1",
        "colab_type": "text"
      },
      "source": [
        "### Stemming\n",
        "perpose of stemming is really depends, we really don't always need to -\n",
        "stemm the tokenized words. it helps finding similar words with different forms "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWWMzjyh73Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II3leXR5Jmls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = PorterStemmer()\n",
        "stemmed_word = []\n",
        "for w in filtered_sentence:\n",
        "  stemmed_word.append(ps.stem(w))\n",
        "\n",
        "print(stemmed_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fb0x-zbTCKq",
        "colab_type": "text"
      },
      "source": [
        "### Part of Speech Taging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPbOJhSCf-9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "3a7606ed-01bd-4eb7-cb9b-84d2308aa5d4"
      },
      "source": [
        "nt.download('state_union')\n",
        "nt.download('punkt')\n",
        "nt.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCvVFC4GLGgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "e50f5666-16e2-4689-f9ac-71daba7ee94d"
      },
      "source": [
        "# nltk imported previously as nt\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw('2006-GWBush.txt')\n",
        "sample_text = combined_sentences\n",
        "\n",
        "# training tokenizer on state union raw text\n",
        "custom_sentence_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "\n",
        "# tokenize the sample text based on training\n",
        "tokenized_sample_text = custom_sentence_tokenizer.tokenize(sample_text)\n",
        "print('Sample_text will be trained on ' + str(len(train_text)) + ' state_union words')\n",
        "\n",
        "tokenized_sample_text\n",
        "\n",
        "# taging process\n",
        "def tag_process():\n",
        "  try:\n",
        "    for i in tokenized_sample_text:\n",
        "      tg_words = nt.word_tokenize(i)\n",
        "      tagged = nt.pos_tag(tg_words)\n",
        "      print(tagged)\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "# calling the tag process\n",
        "tag_process()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample_text will be trained on 33411 state_union words\n",
            "[('It', 'PRP'), ('was', 'VBD'), ('a', 'DT'), ('relatively', 'RB'), ('busy', 'JJ'), ('start', 'NN'), ('to', 'TO'), ('the', 'DT'), ('day', 'NN'), ('on', 'IN'), ('the', 'DT'), ('economic', 'JJ'), ('calendar', 'NN'), ('.', '.')]\n",
            "[('The', 'DT'), ('Japanese', 'JJ'), ('Yen', 'NNP'), ('was', 'VBD'), ('back', 'RB'), ('in', 'IN'), ('action', 'NN'), ('along', 'IN'), ('with', 'IN'), ('the', 'DT'), ('Aussie', 'NNP'), ('Dollar.Away', 'NNP'), ('from', 'IN'), ('the', 'DT'), ('economic', 'JJ'), ('calendar', 'NN'), (',', ','), ('riskier', 'JJR'), ('assets', 'NNS'), ('found', 'VBD'), ('early', 'JJ'), ('support', 'NN'), ('on', 'IN'), ('news', 'NN'), ('of', 'IN'), ('further', 'JJ'), ('progress', 'NN'), ('towards', 'IN'), ('a', 'DT'), ('COVID-19', 'NNP'), ('vaccine.On', 'NN'), ('Monday', 'NNP'), (',', ','), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('coronavirus', 'NN'), ('cases', 'NNS'), ('rose', 'VBD'), ('by', 'IN'), ('182,589', 'CD'), ('to', 'TO'), ('14,850,887', 'CD'), ('.', '.')]\n",
            "[('On', 'IN'), ('Sunday', 'NNP'), (',', ','), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('cases', 'NNS'), ('had', 'VBD'), ('risen', 'VBN'), ('by', 'IN'), ('246,207', 'CD'), ('.', '.')]\n",
            "[('The', 'DT'), ('daily', 'JJ'), ('increase', 'NN'), ('was', 'VBD'), ('lower', 'JJR'), ('than', 'IN'), ('Saturday', 'NNP'), ('’', 'NNP'), ('s', 'VBP'), ('rise', 'NN'), ('and', 'CC'), ('199,164', 'CD'), ('new', 'JJ'), ('cases', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('previous', 'JJ'), ('Monday.Germany', 'NNP'), (',', ','), ('Italy', 'NNP'), (',', ','), ('and', 'CC'), ('Spain', 'NNP'), ('reported', 'VBD'), ('5,413', 'CD'), ('new', 'JJ'), ('cases', 'NNS'), ('on', 'IN'), ('Monday', 'NNP'), (',', ','), ('which', 'WDT'), ('was', 'VBD'), ('up', 'RB'), ('from', 'IN'), ('491', 'CD'), ('new', 'JJ'), ('cases', 'NNS'), ('on', 'IN'), ('Sunday', 'NNP'), ('.', '.')]\n",
            "[('On', 'IN'), ('the', 'DT'), ('previous', 'JJ'), ('Monday', 'NNP'), (',', ','), ('just', 'RB'), ('2,700', 'CD'), ('new', 'JJ'), ('cases', 'NNS'), ('had', 'VBD'), ('been', 'VBN'), ('reported.From', 'VBN'), ('the', 'DT'), ('U.S', 'NNP'), (',', ','), ('the', 'DT'), ('total', 'JJ'), ('number', 'NN'), ('of', 'IN'), ('cases', 'NNS'), ('rose', 'VBD'), ('by', 'IN'), ('62,790', 'CD'), ('to', 'TO'), ('3,961,429', 'CD'), ('on', 'IN'), ('Monday', 'NNP'), ('.', '.')]\n",
            "[('On', 'IN'), ('Sunday', 'NNP'), (',', ','), ('the', 'DT'), ('total', 'JJ'), ('number', 'NN'), ('of', 'IN'), ('cases', 'NNS'), ('had', 'VBD'), ('increased', 'VBN'), ('by', 'IN'), ('65,368', 'CD'), ('.', '.')]\n",
            "[('On', 'IN'), ('Monday', 'NNP'), (',', ','), ('20th', 'CD'), ('July', 'NNP'), (',', ','), ('a', 'DT'), ('total', 'NN'), ('of', 'IN'), ('65,488', 'CD'), ('new', 'JJ'), ('cases', 'NNS'), ('had', 'VBD'), ('been', 'VBN'), ('reported.In', 'VBN'), ('June', 'NNP'), (',', ','), ('core', 'NN'), ('consumer', 'NN'), ('prices', 'NNS'), ('remained', 'VBD'), ('unchanged', 'JJ'), ('compared', 'VBN'), ('with', 'IN'), ('June', 'NNP'), ('2019', 'CD'), ('.', '.')]\n",
            "[('Economists', 'NNS'), ('had', 'VBD'), ('forecast', 'VBN'), ('a', 'DT'), ('0.1', 'CD'), ('%', 'NN'), ('decline.According', 'VBG'), ('to', 'TO'), ('figures', 'NNS'), ('released', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('Ministry', 'NNP'), ('of', 'IN'), ('Internal', 'NNP'), ('Affairs', 'NNPS'), ('and', 'CC'), ('Communication.', 'NNP'), (',', ','), ('the', 'DT'), ('annual', 'JJ'), ('rate', 'NN'), ('of', 'IN'), ('inflation', 'NN'), ('held', 'VBD'), ('steady', 'JJ'), ('at', 'IN'), ('0.1', 'CD'), ('%', 'NN'), ('.The', 'JJ'), ('Japanese', 'JJ'), ('Yen', 'NNP'), ('moved', 'VBD'), ('from', 'IN'), ('¥107.243', 'NN'), ('to', 'TO'), ('¥107.231', 'VB'), ('upon', 'IN'), ('release', 'NN'), ('of', 'IN'), ('the', 'DT'), ('minutes', 'NNS'), ('and', 'CC'), ('stats', 'NNS'), ('.', '.')]\n",
            "[('At', 'IN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Japanese', 'JJ'), ('Yen', 'NN'), ('was', 'VBD'), ('up', 'RB'), ('by', 'IN'), ('0.07', 'CD'), ('%', 'NN'), ('to', 'TO'), ('¥107.19', 'VB'), ('against', 'IN'), ('the', 'DT'), ('U.S', 'NNP'), ('Dollar.The', 'NNP'), ('RBA', 'NNP'), ('Meeting', 'NNP'), ('Minutes', 'NNPS'), ('were', 'VBD'), ('in', 'IN'), ('focus', 'NN'), ('in', 'IN'), ('the', 'DT'), ('late', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('the', 'DT'), ('Asian', 'JJ'), ('session.Salient', 'NN'), ('points', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('minutes', 'NNS'), ('included', 'VBD'), (':', ':'), ('The', 'DT'), ('Aussie', 'NNP'), ('Dollar', 'NNP'), ('moved', 'VBD'), ('from', 'IN'), ('$', '$'), ('0.70267', 'CD'), ('to', 'TO'), ('$', '$'), ('0.70217', 'CD'), ('upon', 'IN'), ('the', 'DT'), ('release', 'NN'), ('of', 'IN'), ('the', 'DT'), ('minutes', 'NNS'), ('.', '.')]\n",
            "[('At', 'IN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Aussie', 'NNP'), ('Dollar', 'NNP'), ('was', 'VBD'), ('up', 'RB'), ('by', 'IN'), ('0.11', 'CD'), ('%', 'NN'), ('to', 'TO'), ('$', '$'), ('0.7024.At', 'CD'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Kiwi', 'NNP'), ('Dollar', 'NNP'), ('was', 'VBD'), ('down', 'RB'), ('by', 'IN'), ('0.02', 'CD'), ('%', 'NN'), ('to', 'TO'), ('$', '$'), ('0.6576.It', 'CD'), ('’', 'NNP'), ('s', 'VBD'), ('a', 'DT'), ('particularly', 'RB'), ('quiet', 'JJ'), ('day', 'NN'), ('ahead', 'RB'), ('on', 'IN'), ('the', 'DT'), ('economic', 'JJ'), ('calendar', 'NN'), ('.', '.')]\n",
            "[('There', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('material', 'JJ'), ('stats', 'NNS'), ('due', 'JJ'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('Eurozone', 'NNP'), ('to', 'TO'), ('provide', 'VB'), ('the', 'DT'), ('EUR', 'NNP'), ('with', 'IN'), ('direction.The', 'JJ'), ('lack', 'NN'), ('of', 'IN'), ('stats', 'NNS'), ('will', 'MD'), ('leave', 'VB'), ('the', 'DT'), ('EUR', 'NNP'), ('in', 'IN'), ('the', 'DT'), ('hands', 'NNS'), ('of', 'IN'), ('updates', 'NNS'), ('from', 'IN'), ('the', 'DT'), ('EU', 'NNP'), ('Recovery', 'NNP'), ('Fund', 'NNP'), ('talks', 'NNS'), ('and', 'CC'), ('market', 'NN'), ('risk', 'NN'), ('sentiment.While', 'VBP'), ('the', 'DT'), ('EU', 'NNP'), ('Recovery', 'NNP'), ('Fund', 'NNP'), ('would', 'MD'), ('be', 'VB'), ('a', 'DT'), ('boost', 'NN'), ('for', 'IN'), ('the', 'DT'), ('struggling', 'VBG'), ('economies', 'NNS'), ('of', 'IN'), ('Italy', 'NNP'), ('and', 'CC'), ('Spain', 'NNP'), (',', ','), ('the', 'DT'), ('coronavirus', 'NN'), ('remains', 'VBZ'), ('a', 'DT'), ('threat', 'NN'), ('.', '.')]\n",
            "[('Progress', 'NN'), ('towards', 'IN'), ('a', 'DT'), ('COVID-19', 'NNP'), ('vaccine', 'NN'), (',', ','), ('however', 'RB'), (',', ','), ('has', 'VBZ'), ('continued', 'VBN'), ('to', 'TO'), ('ease', 'VB'), ('market', 'NN'), ('jitters', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('virus.At', 'NN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('EUR', 'NNP'), ('was', 'VBD'), ('up', 'RB'), ('by', 'IN'), ('0.10', 'CD'), ('%', 'NN'), ('to', 'TO'), ('$', '$'), ('1.1459.It', 'CD'), ('’', 'NNP'), ('s', 'NN'), ('another', 'DT'), ('particularly', 'RB'), ('quiet', 'JJ'), ('day', 'NN'), ('ahead', 'RB'), ('on', 'IN'), ('the', 'DT'), ('economic', 'JJ'), ('calendar', 'NN'), ('.', '.')]\n",
            "[('There', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('material', 'JJ'), ('stats', 'NNS'), ('due', 'JJ'), ('out', 'IN'), ('of', 'IN'), ('the', 'DT'), ('UK', 'NNP'), ('to', 'TO'), ('provide', 'VB'), ('the', 'DT'), ('Pound', 'NNP'), ('with', 'IN'), ('direction.Brexit', 'NN'), ('remains', 'NNS'), ('in', 'IN'), ('focus', 'NN'), ('and', 'CC'), ('will', 'MD'), ('continue', 'VB'), ('to', 'TO'), ('limit', 'VB'), ('any', 'DT'), ('upside', 'NN'), ('in', 'IN'), ('the', 'DT'), ('Pound', 'NNP'), (',', ','), ('barring', 'VBG'), ('the', 'DT'), ('announcement', 'NN'), ('of', 'IN'), ('an', 'DT'), ('agreement', 'NN'), ('on', 'IN'), ('trade.At', 'IN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Pound', 'NN'), ('was', 'VBD'), ('up', 'RB'), ('by', 'IN'), ('0.11', 'CD'), ('%', 'NN'), ('to', 'TO'), ('$', '$'), ('1.2675.It', 'CD'), ('’', 'NNP'), ('s', 'NN'), ('also', 'RB'), ('a', 'DT'), ('particularly', 'RB'), ('quiet', 'JJ'), ('day', 'NN'), ('ahead', 'RB'), ('for', 'IN'), ('the', 'DT'), ('U.S', 'NNP'), ('Dollar', 'NNP'), ('.', '.')]\n",
            "[('There', 'EX'), ('are', 'VBP'), ('no', 'DT'), ('material', 'JJ'), ('stats', 'NNS'), ('due', 'JJ'), ('out', 'RB'), ('to', 'TO'), ('provide', 'VB'), ('the', 'DT'), ('Greenback', 'NNP'), ('with', 'IN'), ('direction.A', 'JJ'), ('lack', 'NN'), ('of', 'IN'), ('stats', 'NNS'), ('will', 'MD'), ('leave', 'VB'), ('the', 'DT'), ('Dollar', 'NN'), ('in', 'IN'), ('the', 'DT'), ('hands', 'NNS'), ('of', 'IN'), ('updates', 'NNS'), ('on', 'IN'), ('COVID-19', 'NNP'), ('and', 'CC'), ('chatter', 'NN'), ('from', 'IN'), ('Washington.At', 'NNP'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Dollar', 'NNP'), ('Spot', 'NNP'), ('Index', 'NNP'), ('was', 'VBD'), ('down', 'VBN'), ('by', 'IN'), ('0.13', 'CD'), ('%', 'NN'), ('to', 'TO'), ('95.705.It', 'CD'), ('’', 'NNS'), ('s', 'VBD'), ('a', 'DT'), ('busier', 'JJ'), ('day', 'NN'), ('ahead', 'RB'), ('on', 'IN'), ('the', 'DT'), ('economic', 'JJ'), ('calendar', 'NN'), ('.', '.')]\n",
            "[('May', 'NNP'), ('retail', 'JJ'), ('sales', 'NNS'), ('and', 'CC'), ('June', 'NNP'), ('new', 'JJ'), ('house', 'NN'), ('price', 'NN'), ('figures', 'NNS'), ('are', 'VBP'), ('due', 'JJ'), ('out', 'IN'), ('of', 'IN'), ('Canada', 'NNP'), ('later', 'RB'), ('today.Barring', 'VBG'), ('a', 'DT'), ('“', 'JJ'), ('risk-off', 'NN'), ('”', 'NNP'), ('event', 'NN'), (',', ','), ('we', 'PRP'), ('would', 'MD'), ('expect', 'VB'), ('the', 'DT'), ('retail', 'JJ'), ('sales', 'NNS'), ('figures', 'NNS'), ('to', 'TO'), ('have', 'VB'), ('the', 'DT'), ('greatest', 'JJS'), ('impact.The', 'NN'), ('markets', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('rebound', 'NN'), ('from', 'IN'), ('April', 'NNP'), ('’', 'NNP'), ('s', 'VBD'), ('tumble.Away', 'RB'), ('from', 'IN'), ('the', 'DT'), ('stats', 'NNS'), (',', ','), ('any', 'DT'), ('rise', 'NN'), ('in', 'IN'), ('tension', 'NN'), ('between', 'IN'), ('the', 'DT'), ('U.S', 'NNP'), ('and', 'CC'), ('China', 'NNP'), ('will', 'MD'), ('need', 'VB'), ('consideration', 'NN'), ('along', 'IN'), ('with', 'IN'), ('COVID-19', 'NNP'), ('updates.At', 'IN'), ('the', 'DT'), ('time', 'NN'), ('of', 'IN'), ('writing', 'NN'), (',', ','), ('the', 'DT'), ('Loonie', 'NNP'), ('was', 'VBD'), ('up', 'RB'), ('by', 'IN'), ('0.13', 'CD'), ('%', 'NN'), ('to', 'TO'), ('C', 'NNP'), ('$', '$'), ('1.3518', 'CD'), ('against', 'IN'), ('the', 'DT'), ('U.S', 'NNP'), ('Dollar.For', 'NNP'), ('a', 'DT'), ('look', 'NN'), ('at', 'IN'), ('all', 'DT'), ('of', 'IN'), ('today', 'NN'), ('’', 'NNP'), ('s', 'VBP'), ('economic', 'JJ'), ('events', 'NNS'), (',', ','), ('check', 'VB'), ('out', 'RP'), ('our', 'PRP$'), ('economic', 'JJ'), ('calendar.This', 'NN'), ('article', 'NN'), ('was', 'VBD'), ('originally', 'RB'), ('posted', 'VBN'), ('on', 'IN'), ('FX', 'NNP'), ('Empire', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kAMXdJVU4K2",
        "colab_type": "text"
      },
      "source": [
        "## Chunking\n",
        "*Chunking is basically grab specific data using RegEx\n",
        "Here we will mostly be using Modifier.\\*\n",
        "**Meta Charecters** --> .[{()\\^$|?*\n",
        "- dot(.) matches any character except newline\n",
        "- escape character (\\\\) escape any character after it\n",
        "- (\\d) matches digit 0-9\n",
        "- (\\D) matches everything except 0-9\n",
        "- (\\w) matches word character (a-z, A-Z, 0-9, _)\n",
        "- (\\W) matches everythong exept word character\n",
        "- (\\s) whitespaces (space, tab, newline)\n",
        "- (\\S) Everything except Whitespaces\n",
        "- (\\b) Word Boundary\n",
        "- (\\B) Not a word boundary\n",
        "_____________________________________________\n",
        "\n",
        "- ([ ]) Matches Chracter in the Brackets\n",
        "- ([^]) Matches everything except what in the Bracket\n",
        "- (^)  Beging of string\n",
        "- ($) End of string\n",
        "- (|) Either Or"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW5Rjvdjf7Xe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}